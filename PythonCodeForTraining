(I) Face Detection and Recognition :

1) DataSets :

#OpenCV2 for image processing.
import cv2

#numpy for numerical and scientific computing.
import numpy as np
from pip._vendor.distlib.compat import raw_input

#"haarcascade_frontalface_default.xml" is pre-built frontal face training model from OpenCV using for Face detection.
faceDetect = cv2.CascadeClassifier('haarcascade_frontalface_default.xml');

#Starting Video Capturing and 0 indicates the default value for webcam.
cam = cv2.VideoCapture(0);

#id, sampleNum for labelling the dataSets.
id = raw_input("Enter your ID : ")
sampleNum = 0;

while(True):

    #Capturing Video Frame.
    ret, img = cam.read();

    #Converting the frame to grayScale.
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    #Detect frames of different sizes, list of faces rectangles.
    faces = faceDetect.detectMultiScale(gray, 1.3, 5);

    #Loop for face detected in each frame.
    for(x,y,w,h) in faces :

        #incremented for each face(dataset) of a person.
        sampleNum = sampleNum + 1;

        #Saving the captured images into DataSet folder.
        cv2.imwrite("dataSet/User." + str(id) + "." + str(sampleNum) + ".jpg", gray[y:y + h, x:x + w])

        #Creating rectanglar boundary around the detected face.
        cv2.rectangle(img, (x,y), (x+w, y+h), (0,0,255), 2)

        #Time to each for capturing pictures.
        cv2.waitKey(100);

    #Display the video frame with the rectagle represented around the face above.
    cv2.imshow("Face", img);


    cv2.waitKey(1)

    #After saving 20 pictures, exit the loop.
    if(sampleNum > 150) :
        break;

#Stop video camera.
cam.release()

#Close all windows.
cv2.destroyAllWindows()

______________________________________________________________________________________________________

2) Training the dataset :

#OS for file path.
import os

#OpenCV2 for image processing.
import cv2

#numpy for numerical and scientific computing.
import numpy as np

#PIL - Python Image Library.
from PIL import Image

#If path exists, select it else create it.
def assure_path_exists(path):
    dir = os.path.dirname(path)
    if not os.path.exists(dir):
        os.makedirs(dir)

#Create Local Binary Pattern Histograms for Face Recognizing.
recognizer = cv2.face.LBPHFaceRecognizer_create()

#"haarcascade_frontalface_default.xml" is pre-built frontal face training model from OpenCV using for Face detection.
detector = cv2.CascadeClassifier("haarcascade_frontalface_default.xml");

#Set path as dataSet.
path = 'dataSet'

#Function to get the images with label data.
def getImagesAndLabels(path):

    #Get all file path.
    imagePaths = [os.path.join(path, f) for f in os.listdir(path)]

    #For faces.
    faceSamples = []

    #For IDs.
    ids = []

    #Loop for all the file path.
    for imagePath in imagePaths:

        #Convert the image to gray scale.
        PIL_img = Image.open(imagePath).convert('L')

        #Convert PIL image to Numpy Array.
        img_numpy = np.array(PIL_img, 'uint8')

        #Image ID.
        id = int(os.path.split(imagePath)[-1].split(".")[1])

        #Get face from training images.
        faces = detector.detectMultiScale(img_numpy)

        #Loop for faces and add the corresponding IDs.
        for (x, y, w, h) in faces:
            faceSamples.append(img_numpy[y:y + h, x:x + w])
            ids.append(id)
    return faceSamples, ids

#Get faces and IDs.
faces,ids = getImagesAndLabels('dataset')

#Training model using faces and IDs.
recognizer.train(faces, np.array(ids))

#Checking for training folder.
assure_path_exists('trainer/')

#Save the model into trainer.yml
recognizer.save('trainer/trainer.yml')


__________________________________________________________________________________________________

3) Face Recognition :

#OpenCV2 for image processing.
import cv2

#OS for file path.
import os

#numpy for numerical and scientific computing.
import numpy as np

#If path exists, select it else create it.
def assure_path_exists(path):
    dir = os.path.dirname(path)
    if not os.path.exists(dir):
        os.makedirs(dir)

#Create Local Binary Pattern Histograms for Face Recognizing.
recognizer = cv2.face.LBPHFaceRecognizer_create()

#Check for trainer folder existance.
assure_path_exists("trainer/")

#Getting the trained model.
recognizer.read('trainer/trainer.yml')

#"haarcascade_frontalface_default.xml" is pre-built frontal face training model from OpenCV using for Face detection.
faceDetect = cv2.CascadeClassifier('haarcascade_frontalface_default.xml');

font = cv2.FONT_HERSHEY_SIMPLEX

#Starting Video Capturing and 0 indicates the default value for webcam.
cam = cv2.VideoCapture(0);


while(True):

    #Capturing Video Frame.
    ret, img = cam.read();

    #Converting the frame to grayScale.
    gray = cv2.cvtColor(img, cv2.COLOR_BGR2GRAY)

    #Detect frames of different sizes, list of faces rectangles.
    faces = faceDetect.detectMultiScale(gray, 1.3, 5);

    #Loop for face detected in each frame.
    for(x,y,w,h) in faces :

        #Creates rectangle around face.
        cv2.rectangle(img, (x,y), (x+w, y+h), (0,0,255), 4)

        #Recognize the face ID.
        Id, confidence = recognizer.predict(gray[y:y + h, x:x + w])

        #Check for ID.
        if (Id == 1):
            Id = "Praveen {0:.2f}%".format(round(100 - confidence, 2))
        elif(Id == 2):
            Id = "Karthik {0:.2f}%".format(round(100 - confidence), 2)
        elif(Id == 3) :
            Id = "JS R {0:.2f}%".format(round(100 - confidence), 2)
        elif(Id == 4) :
            Id = "Mujahed {0:.2f}%".format(round(100 - confidence), 2)

        #Naming the recognized face.
        cv2.rectangle(img, (x - 22, y - 90), (x + w + 22, y - 22), (0, 255, 0), -1)
        cv2.putText(img, str(Id), (x, y - 40), font, 1, (255, 255, 255), 3)

    #Show video frame with bounded attributes.
    cv2.imshow("Face", img);

    #Press 'q' to quit.
    if(cv2.waitKey(1) == ord('q')) :
        break;

#Close Video Camera.
cam.release()

#Close all the windows.
cv2.destroyAllWindows()

_____________________________________________________________________________________________________

(II) Emotion Recognition :

1) Covert CSV to Images :

'''
This script creates 3-channel gray images from FER 2013 dataset.

It has been done so that the CNNs designed for RGB images can 
be used without modifying the input shape. 

This script requires two command line parameters:
1. The path to the CSV file
2. The output directory

It generates the images and saves them in three directories inside 
the output directory - Training, PublicTest, and PrivateTest. 
These are the three original splits in the dataset. 
'''

import os
import csv
import argparse
import numpy as np 
import scipy.misc

from PIL import Image

parser = argparse.ArgumentParser()
parser.add_argument('-f', '--file', required=True, help="path of the csv file")
parser.add_argument('-o', '--output', required=True, help="path of the output directory")
args = parser.parse_args()

w, h = 48, 48
image = np.zeros((h, w), dtype=np.uint8)
id = 1

with open(args.file) as csvfile:
    datareader = csv.reader(csvfile, delimiter =',')
    next(datareader,None)
	
    for row in datareader:
        
        emotion = row[0]
        pixels = row[1].split()
        usage = row[2]
        pixels_array = np.asarray(pixels, dtype=np.int)

        image = pixels_array.reshape(w, h)
        #print image.shape

        stacked_image = np.dstack((image,) * 3)
        #print stacked_image.shape

        image_folder = os.path.join(args.output, usage)
        if not os.path.exists(image_folder):
            os.makedirs(image_folder)
        image_file =  os.path.join(image_folder , str(id)+'_'+emotion+'.jpg')
        scipy.misc.imsave(image_file, stacked_image)
        id += 1 
        if id % 100 == 0:
            print('Processed {} images'.format(id))

print("Finished processing {} images".format(id))


________________________________________________________________________________________________________________________

2) Train emotion class :

"""
Description: Train emotion classification model
"""

from keras.callbacks import CSVLogger, ModelCheckpoint, EarlyStopping
from keras.callbacks import ReduceLROnPlateau
from keras.preprocessing.image import ImageDataGenerator
from sklearn.model_selection import train_test_split
from keras.layers import Activation, Convolution2D, Dropout, Conv2D
from keras.layers import AveragePooling2D, BatchNormalization
from keras.layers import GlobalAveragePooling2D
from keras.models import Sequential
from keras.layers import Flatten
from keras.models import Model
from keras.layers import Input
from keras.layers import MaxPooling2D
from keras.layers import SeparableConv2D
from keras import layers
from keras.regularizers import l2
import pandas as pd
import cv2
import numpy as np

dataset_path = 'fer2013/fer2013.csv'
image_size=(48,48)
# parameters
batch_size = 32
num_epochs = 110
input_shape = (48, 48, 1)
validation_split = .2
verbose = 1
num_classes = 7
patience = 50
base_path = 'models/'
l2_regularization=0.01


def load_fer2013():
	data = pd.read_csv(dataset_path)
	pixels = data['pixels'].tolist()
	width, height = 48, 48
	faces = []
	for pixel_sequence in pixels:
		face = [int(pixel) for pixel in pixel_sequence.split(' ')]
		face = np.asarray(face).reshape(width, height)
		face = cv2.resize(face.astype('uint8'),image_size)
		faces.append(face.astype('float32'))
	faces = np.asarray(faces)
	faces = np.expand_dims(faces, -1)
	emotions = pd.get_dummies(data['emotion']).as_matrix()
	return faces, emotions

def preprocess_input(x, v2=True):
    x = x.astype('float32')
    x = x / 255.0
    if v2:
        x = x - 0.5
        x = x * 2.0
    return x



# data generator
data_generator = ImageDataGenerator(
                        featurewise_center=False,
                        featurewise_std_normalization=False,
                        rotation_range=10,
                        width_shift_range=0.1,
                        height_shift_range=0.1,
                        zoom_range=.1,
                        horizontal_flip=True)

# model parameters/compilation
# model = mini_XCEPTION(input_shape, num_classes)
regularization = l2(l2_regularization)

# base
img_input = Input(input_shape)
x = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization, use_bias=False)(img_input)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = Conv2D(8, (3, 3), strides=(1, 1), kernel_regularizer=regularization, use_bias=False)(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)

# module 1
residual = Conv2D(16, (1, 1), strides=(2, 2), padding='same', use_bias=False)(x)
residual = BatchNormalization()(residual)
x = SeparableConv2D(16, (3, 3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = SeparableConv2D(16, (3, 3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)
x = BatchNormalization()(x)
x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)
x = layers.add([x, residual])

# module 2
residual = Conv2D(32, (1, 1), strides=(2, 2), padding='same', use_bias=False)(x)
residual = BatchNormalization()(residual)
x = SeparableConv2D(32, (3, 3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = SeparableConv2D(32, (3, 3), padding='same', kernel_regularizer=regularization, use_bias=False)(x)
x = BatchNormalization()(x)
x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)
x = layers.add([x, residual])

# module 3
residual = Conv2D(64, (1, 1), strides=(2, 2),padding='same', use_bias=False)(x)
residual = BatchNormalization()(residual)
x = SeparableConv2D(64, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = SeparableConv2D(64, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)
x = BatchNormalization()(x)
x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)
x = layers.add([x, residual])

# module 4
residual = Conv2D(128, (1, 1), strides=(2, 2),padding='same', use_bias=False)(x)
residual = BatchNormalization()(residual)
x = SeparableConv2D(128, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)
x = BatchNormalization()(x)
x = Activation('relu')(x)
x = SeparableConv2D(128, (3, 3), padding='same',kernel_regularizer=regularization,use_bias=False)(x)
x = BatchNormalization()(x)
x = MaxPooling2D((3, 3), strides=(2, 2), padding='same')(x)
x = layers.add([x, residual])
x = Conv2D(num_classes, (3, 3), padding='same')(x)
x = GlobalAveragePooling2D()(x)
output = Activation('softmax',name='predictions')(x)

model = Model(img_input, output)
model.compile(optimizer='adam', loss='categorical_crossentropy',metrics=['accuracy'])
model.summary()

# callbacks
log_file_path = base_path + '_emotion_training.log'
csv_logger = CSVLogger(log_file_path, append=False)
early_stop = EarlyStopping('val_loss', patience=patience)
reduce_lr = ReduceLROnPlateau('val_loss', factor=0.1, patience=int(patience/4), verbose=1)
trained_models_path = base_path + '_mini_XCEPTION'
model_names = trained_models_path + '.{epoch:02d}-{val_acc:.2f}.hdf5'
model_checkpoint = ModelCheckpoint(model_names, 'val_loss', verbose=1,save_best_only=True)
callbacks = [model_checkpoint, csv_logger, early_stop, reduce_lr]

# loading dataset
faces, emotions = load_fer2013()
faces = preprocess_input(faces)
num_samples, num_classes = emotions.shape
xtrain, xtest,ytrain,ytest = train_test_split(faces, emotions,test_size=0.2,shuffle=True)
model.fit_generator(data_generator.flow(xtrain, ytrain,
                                            batch_size),
                        steps_per_epoch=len(xtrain) / batch_size,
                        epochs=num_epochs, verbose=1, callbacks=callbacks,
                        validation_data=(xtest,ytest))
___________________________________________________________________________________________________________________________

3) Recognise emotion by giving image as input :

from keras.preprocessing.image import img_to_array
from keras.models import load_model
import imutils
import cv2
import numpy as np
import sys

# parameters for loading data and images
detection_model_path = 'haarcascade_files/haarcascade_frontalface_default.xml'
emotion_model_path = 'models/_mini_XCEPTION.106-0.65.hdf5'
img_path = sys.argv[1]

# hyper-parameters for bounding boxes shape
# loading models
face_detection = cv2.CascadeClassifier(detection_model_path)
emotion_classifier = load_model(emotion_model_path, compile=False)
EMOTIONS = ["angry","disgust","scared", "happy", "sad", "surprised","neutral"]


#reading the frame
orig_frame = cv2.imread(img_path) 
frame = cv2.imread(img_path,0)
faces = face_detection.detectMultiScale(frame,scaleFactor=1.1,minNeighbors=5,minSize=(30,30),flags=cv2.CASCADE_SCALE_IMAGE)

if len(faces) > 0:
    faces = sorted(faces, reverse=True,key=lambda x: (x[2] - x[0]) * (x[3] - x[1]))[0]
    (fX, fY, fW, fH) = faces
    roi = frame[fY:fY + fH, fX:fX + fW]
    roi = cv2.resize(roi, (48, 48))
    roi = roi.astype("float") / 255.0
    roi = img_to_array(roi)
    roi = np.expand_dims(roi, axis=0)
    preds = emotion_classifier.predict(roi)[0]
    emotion_probability = np.max(preds)
    label = EMOTIONS[preds.argmax()]
    cv2.putText(orig_frame, label, (fX, fY - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)
    cv2.rectangle(orig_frame, (fX, fY), (fX + fW, fY + fH),(0, 0, 255), 2)
    
cv2.imshow('test_face', orig_frame)
cv2.imwrite('test_output/'+img_path.split('/')[-1],orig_frame)
if (cv2.waitKey(2000000) & 0xFF == ord('q')):
    sys.exit("Thanks")
cv2.destroyAllWindows()

_________________________________________________________________________________________________________________________________________

4) Real time emotion detection :

from keras.preprocessing.image import img_to_array
import imutils
import cv2
from keras.models import load_model
import numpy as np

# parameters for loading data and images
detection_model_path = 'haarcascade_files/haarcascade_frontalface_default.xml'
emotion_model_path = 'models/_mini_XCEPTION.106-0.65.hdf5'

# hyper-parameters for bounding boxes shape
# loading models
face_detection = cv2.CascadeClassifier(detection_model_path)
emotion_classifier = load_model(emotion_model_path, compile=False)
EMOTIONS = ["angry" ,"disgust","scared", "happy", "sad", "surprised",
 "neutral"]


# starting video streaming
cv2.namedWindow('your_face')
camera = cv2.VideoCapture(0)
while True:
    frame = camera.read()[1]
    #reading the frame
    frame = imutils.resize(frame,width=400)
    gray = cv2.cvtColor(frame, cv2.COLOR_BGR2GRAY)
    faces = face_detection.detectMultiScale(gray,scaleFactor=1.1,minNeighbors=5,minSize=(30,30),flags=cv2.CASCADE_SCALE_IMAGE)
    
    canvas = np.zeros((250, 300, 3), dtype="uint8")
    frameClone = frame.copy()

    preds=[]
    
    if len(faces) > 0:
        faces = sorted(faces, reverse=True,
        key=lambda x: (x[2] - x[0]) * (x[3] - x[1]))[0]
        (fX, fY, fW, fH) = faces
                    # Extract the ROI of the face from the grayscale image, resize it to a fixed 48x48 pixels, and then prepare
            # the ROI for classification via the CNN
        roi = gray[fY:fY + fH, fX:fX + fW]
        roi = cv2.resize(roi, (48, 48))
        roi = roi.astype("float") / 255.0
        roi = img_to_array(roi)
        roi = np.expand_dims(roi, axis=0)
        
        
        preds = emotion_classifier.predict(roi)[0]
        emotion_probability = np.max(preds)
        label = EMOTIONS[preds.argmax()]

 
    for (i, (emotion, prob)) in enumerate(zip(EMOTIONS, preds)):
                # construct the label text
                text = "{}: {:.2f}%".format(emotion, prob * 100)
                w = int(prob * 300)
                cv2.rectangle(canvas, (7, (i * 35) + 5),
                (w, (i * 35) + 35), (0, 0, 255), -1)
                cv2.putText(canvas, text, (10, (i * 35) + 23),
                cv2.FONT_HERSHEY_SIMPLEX, 0.45,
                (255, 255, 255), 2)
                cv2.putText(frameClone, label, (fX, fY - 10),
                cv2.FONT_HERSHEY_SIMPLEX, 0.45, (0, 0, 255), 2)
                cv2.rectangle(frameClone, (fX, fY), (fX + fW, fY + fH),
                              (0, 0, 255), 2)

    cv2.imshow('your_face', frameClone)
    cv2.imshow("Probabilities", canvas)
    if cv2.waitKey(1) & 0xFF == ord('q'):
        break

camera.release()
cv2.destroyAllWindows()

____________________________________________________________________________________________________________________

